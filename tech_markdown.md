## 1. 데이터 수집

<br>

Bruch Networking 프로젝트를 진행하며 경험한 데이터 수집-정제-분석-적용 전체 과정을 설명하고자 합니다. 전체 code는 sourceCode 섹션에서 확인하실 수 있습니다.

<img src = "https://user-images.githubusercontent.com/35517797/80509879-f1149d00-89b4-11ea-939d-5eb1af734319.png" height="300" width="650px">

<br>
<br>

먼저, 프로젝트를 위해 필요한 데이터를 정의합니다. text 자동 분류와 추천시스템 구현이 목적이므로 아래와 같이 필요한 데이터를 정의했습니다.

- 전체 20개 카테고리의 각 카테고리별 게시글
- 게시글 별 정보(제목,발행일,공유횟수,좋아요수,keyword,댓글 ...)

<br>

이렇게 필요 데이터를 정의한 후 본격적으로 크롤링 코드를 구현합니다. 브런치의 카테고리별 게시글page의 경우 "무한스크롤" javaScript가 구현되어 있습니다.
javaScript를 제어하기 위해서는 BeautifulSoup만으로는 불가능하므로 시간이 더 소요될 수 있지만 Selenium패키지를 사용해야 합니다. 크롤링 과정은 크게 다음과 같습니다.

1. 카테고리별 게시글 목록 page => 게시글 별 작가 id(url) parsing
2. 각 게시글 page => text, 제목, 발행일 등의 정보를 parsing <br>

(* page => 해당 page에서 수집하려는 정보)

<br>

카테고리별로 게시글의 수가 다르기 때문에 무한스크롤의 횟수를 조건으로 각각 어느정도의 데이터를 수집하였습니다. 수집한 데이터를 간단한 전처리를 거쳐 DataFrame형식으로
만들었고 아래의 이미지와 같습니다. 전처리 부분은 바로 이어서 설명하겠습니다.

<img src = "https://user-images.githubusercontent.com/35517797/80506298-587c1e00-89b0-11ea-9602-c63c1526b109.PNG" height="330" width="700px">

<br>
<br>

## 2. Text 데이터 전처리

<br>

크롤링 첫 단계에서 수집된 데이터는 여러가지 "잔"처리를 해주어야 합니다. 제가 만든 크롤러의 경우에 2단계의 과정을 거쳐야 하기 때문에 pickle과 json형식을 주로 사용하였으며, pandas 라이브러리를 주로 활용하였습니다. 이제 기초적인 "잔"처리 단계가 끝나면 위 이미지와 같은 데이터 형태를 얻게되고, text분석을 위한 전처리 단계에 돌입해야 합니다.

### 2.1 토큰화(Tokenization)

토큰화란, 분석하고자하는 문서를 문장 혹은 단어 단위로 분리해내는것을 의미합니다.

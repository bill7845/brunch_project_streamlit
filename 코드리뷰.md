# Brunch Networking Tech
---

<br>

## < 목차 >

[1. 데이터 수집](#데이터-수집) <br> 
[2. 데이터 전처리](#데이터-전처리) <br>



<!--
* 1. 데이터 수집
* 2. 데이터 전처리
  - 2-1. 데이터 클렌징
  - 2-2. 텍스트 데이터 전처리
    - 2-2-1 text cleaning
    - 2-2-2 text tokenize & vectorize
* 3. 분류 모델
  - 평가
* 4. 추천 시스템
  - 4.1 입력 데이터 기반
  - 4.2 키워드 기반
* 5. streamlit을 활용한 API <br>
-->

---

<br>

## <b><데이터 수집></b>
<br>

Bruch Networking 프로젝트를 진행하며 경험한 데이터 수집-전처리-분석-모델링의 전체 과정을 코드와 함께 설명하고자 합니다.

<br>

<!-- <img src = "https://user-images.githubusercontent.com/35517797/80509879-f1149d00-89b4-11ea-939d-5eb1af734319.png" height="300" width="650px"> -->


먼저, 프로젝트를 위해 필요한 데이터를 정의합니다. 프로젝트의 주된 목표는 `text 자동분류`, `text 추천시스템` 2가지입니다. 이에 맞춰 필요한 데이터를 우선적으로 정의합니다.

<br>

* <b> Brunch에 게시된 게시글(20개 카테고리별) </b>
* <b> 각 게시글 별 정보(제목,발행일,공유횟수,좋아요수,keyword,댓글 ...) </b>

<br>

이렇게 필요 데이터를 정의한 후 크롤링 코드를 구합니다. 브런치의 카테고리별 게시글page의 경우 "무한스크롤" javaScript가 구현되어 있습니다.
javaScript를 제어하기 위해서는 BeautifulSoup만으로는 불가능하므로 시간이 더 소요될 수 있지만 Selenium패키지를 사용해야 합니다. 전체 크롤링 과정을 요약하면 크게 2단계로 요약할 수 있습니다.

<br>

* <b> 1. 카테고리별 게시글 목록 page => 게시글 별 작가 id(url) parsing </b> <br>
* <b> 2. 각 게시글 page => text, 제목, 발행일 등의 정보를 parsing </b>

<br>

<img src = "https://user-images.githubusercontent.com/35517797/81310986-c5359d80-90bf-11ea-874c-7473a9b910b0.PNG" height="300" width="650px">

<br>
<br>

2단계의 크롤링을 코드로 구현합니다.

<br>

~~~python
###################################################################
########################### 크롤링 1단계 ###########################
#####카테고리별 게시글 목록 page => 게시글 별 작가 id(url) parsing ###
###################################################################
import time
import requests
import pickle
from bs4 import BeautifulSoup

# 카테고리 별 게시글 리스트 페이지에서 유저별 id 파싱 (*전체 24개 카테고리)
# 각 페이지별 무한 스크롤 javaScript 제어를 위한 셀레니움 기능 사용
def get_user_list(base_url):

    # 크롬드라이버 설정
    chromedriver = 'C:/selenium/chromedriver.exe' # chromedriver path
    driver = webdriver.Chrome(chromedriver)
    driver.get(base_url) # url acess

    SCROLL_PAUSE_TIME = 10 # 무한스크롤 멈춤 현상 예방
    last_height = driver.execute_script("return document.body.scrollHeight")
    i = 0
    while True:
        # Scroll down to bottom                                                      
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        # Wait to load page
        time.sleep(SCROLL_PAUSE_TIME)                                                
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight-50);")  
        time.sleep(SCROLL_PAUSE_TIME)

        # Calculate new scroll height and compare with last scroll height            
        new_height = driver.execute_script("return document.body.scrollHeight")
        i+=1

        if i == 350: # 각 카테고리별 스크롤 횟수를 350번으로 제어                                                
            break
        last_height = new_height

    # 각 페이별 350번 스크롤 후, BeautifulSoup으로 "전체 게시글 url parsing"
    html = driver.page_source
    soup = BeautifulSoup(html,'html.parser')
    a_tags = soup.select('#wrapArticle > div.wrap_article_list.\#keyword_related_contents > ul > li > a.link_post')

    save_href = []
    for a_tag in a_tags :
        save_href.append(a_tag['href'])

    driver.close()

    return save_href # 개별 카테고리별 전체 게시글 url 반환

category_list = ['지구한바퀴_세계여행?q=g','그림·웹툰?q=g','시사·이슈?q=g','IT_트렌드?q=g','사진·촬영?q=g','',
                 '취향저격_영화_리뷰?q=g','오늘은_이런_책?q=g','뮤직_인사이드?q=g','글쓰기_코치?q=g','직장인_현실_조언?q=g'
                 ,'스타트업_경험담?q=g','육아_이야기?q=g','요리·레시피?q=g','건강·운동?q=g','멘탈_관리_심리_탐구?q=g',
                 '디자인_스토리?q=g','문화·예술?q=g','건축·설계?q=g','인문학·철학?q=g','쉽게_읽는_역사?q=g','우리집_반려동물?q=g',
                 '멋진_캘리그래피?q=g','사랑·이별?q=g','감성_에세이?q=g']


# 전체 24개 카테고리별로 돌며 게시글 url 파싱하여 pickle로 저장
for category in category_list:
    each_user_id = []
    each_user_id = get_user_list("https://brunch.co.kr/keyword/"+category)
    with open(category[:-4]+'_userId.txt','wb') as f:
        pickle.dump(each_user_id,f)
~~~

<br>

1단계 크롤링의 결과로 20개 카테고리별 게시글 url을 담은 리스트를 얻었습니다. 이제 각 게시글 정보를 수집하여 json 파일로 저장합니다.

<br>

~~~python
###################################################################
########################### 크롤링 2단계 ###########################
###   각 게시글 page => text, 제목, 발행일 등의 정보를 parsing ######
###################################################################
## url pickle load
pickles = ['지구한바퀴_세계여행?q=g','그림·웹툰?q=g','시사·이슈?q=g','IT_트렌드?q=g','사진·촬영?q=g',
                 '취향저격_영화_리뷰?q=g','오늘은_이런_책?q=g','뮤직_인사이드?q=g','글쓰기_코치?q=g','직장인_현실_조언?q=g'
                 ,'스타트업_경험담?q=g','육아_이야기?q=g','요리·레시피?q=g','건강·운동?q=g','멘탈_관리_심리_탐구?q=g',
                 '디자인_스토리?q=g','문화·예술?q=g','건축·설계?q=g','인문학·철학?q=g','쉽게_읽는_역사?q=g','우리집_반려동물?q=g',
                 '멋진_캘리그래피?q=g','사랑·이별?q=g','감성_에세이?q=g']

writer_list = []
for file in pickles:
#     print(file)
    with open('~~path~~/'+file[:-4]+"_userId.txt","rb") as fr:
        writers = pickle.load(fr)
    writer_list.append(writers)  ## [[카테고리1 게시글 url...],[카테고리2 게시글 url], ....[카테고리24 게시글 url]]

## 게시글 속 정보 수집
def def_craw(writer):
    json_data = {}
    data = []
    res_text = []
    tag_keyword=[]

    tag_title,tag_nickname,tag_publish_date,tag_url,tag_url_plink = None,None,None,None,None
    tag_share,tag_like = str,str
    for url in writer:
        if res_text == []: # 첫 시작에러 방지
            pass
        else :
            # to json
            json_data['title'] = tag_title  
            json_data['nickname'] = tag_nickname
            json_data['publish_date'] = tag_publish_date
            json_data['keyword'] = tmp_keyword   
            json_data['like'] = tag_like # like 없는 경우 ''
            json_data['share'] = tag_share # share 없는 경우 None
            json_data['comment'] = tag_comment # comment 없는 경우 ''
            json_data['url'] = tag_url
            json_data['url_plink'] = tag_url_plink
            json_data['text'] = res_text

        data.append(json_data)

        json_data = {} # 누적방지 초기화
        tmp_keyword = [] # 누적방지 초기화
        res_text = [] # 누적방지 초기화

        # beautifulsoup
        html = requests.get('https://brunch.co.kr{text_url}'.format(text_url=url))
        soup = BeautifulSoup(html.text, 'html.parser')

        if soup.find('title').text == "brunch":
            pass
        else:
            tag_title = soup.find('title').text # 게시글 title
            tag_url = soup.find("meta",property='og:url')['content'] # 게시글 본주소
            tag_nickname = soup.find("meta",{'name':'article:media_name'})['content'] # 작가 nickname
            tag_url_plink = soup.find("meta",property='dg:plink')['content'] # 암호주소? # 모바일?
            tag_publish_date = soup.find("meta",property='article:published_time')['content'] # 발행일
            tag_keyword = soup.find_all('a',href=re.compile('/keyword')) # 게시글 키워드
            tag_like = soup.find('span',{'class':'f_l text_like_count text_default text_with_img_ico ico_likeit_like #like'}) #좋아요 수
            tag_share = soup.find('span',{'class':'f_l text_share_count text_default text_with_img_ico'}) # 공유 수
            tag_comment = soup.find('span',{'class':'f_l text_comment_count text_default text_with_img_ico'}) # 댓글 수
            text_h4 = soup.find_all(class_='wrap_item item_type_text')

            for text in text_h4:
                res_text.append(text.text)

            if tag_like == None:
                tag_like = "0"
            else:
                tag_like = tag_like.text # 좋아요 수

            if tag_share == None:
                tag_share == "0"
            else:
                tag_share = tag_share.text # 공유 수

            if tag_comment == None:
                tag_comment =="0"
            else:
                tag_comment = tag_comment.text

            for keyword in tag_keyword:
                tmp_keyword.append(keyword.text)

    return data ## 수집한 정보를 담은 dictionary로 반환

# categories = ['지구한바퀴_세계여행','그림·웹툰','시사·이슈','IT_트렌드','사진·촬영',
#                  '취향저격_영화_리뷰','오늘은_이런_책','뮤직_인사이드','글쓰기_코치','직장인_현실_조언'
#                  ,'스타트업_경험담','육아_이야기','요리·레시피','건강·운동','멘탈_관리_심리_탐구',
#                  '디자인_스토리','문화·예술','건축·설계','인문학·철학','쉽게_읽는_역사','우리집_반려동물',
#                  '멋진_캘리그래피','사랑·이별','감성_에세이']


## 카테고리 -> 게시글의 순서로 2차 크롤링 진행
## 카테고리별로 정보를 담은 json 형식으로 저장(총 24개 json file)
for idx,writer in enumerate(writer_list):
    to_json = None
    data = def_craw(writer) # 2단계 크롤링 실행

    del data[0]
    del data[0]

    to_json = OrderedDict()
    to_json['name'] = categories[idx] # category name
    to_json['version'] = "2020-04-21"
    to_json['data'] = data

    with open(categories[idx]+".json","w") as make_file:
        json.dump(to_json,make_file)
~~~

<br>

이제 필요한 데이터 수집이 완료되었습니다. "멋진 캘리그래피"의 수집결과를 확인해 보겠습니다. 수집한 데이터는 json 형식으로 저장하였습니다.

<br>

<img src = "https://user-images.githubusercontent.com/35517797/81311751-c1564b00-90c0-11ea-9e04-63f470549612.PNG" height="330" width="700px">

<br><br>

불러온 json데이터를 분석에 편리하도록 pandas의 DataFrame 형식으로 변환해 줍니다.

<br>

~~~python
df = pd.DataFrame(json_data['data'],
                  columns=['title','keyword','text','nickname','publish_date','likes','share','comment','url','url_plink'])

df.head(3)
~~~

<br>

<img src = "https://user-images.githubusercontent.com/35517797/80673586-a432e280-8aea-11ea-9ac2-083ae8167876.PNG" height="330" width="700px">

<br><br>

<br>

## <b><데이터 전처리></b>

<br>

DataFrame형식으로 불러왔음에도 아직 지저분한 부분이 많이 있습니다. 본격적인 Text 데이터 전처리에 앞서 데이터 "잔처리"를 진행해줍니다. 아울러 이 작업을 24개 전체 카테고리에 한번에 적용합니다.

* <b> ext column : 결측값 삭제 , 기존 문장단위의 리스트 형식에서 전체 문자열 형식으로 변환 </b>
* <b> keyword column : \n, 공백 제거 후 리스트 형식으로 변환 </b>
* <b> comment column : comment가 없는경우 공백이 아닌 Nan으로 변환 </b>
* <b> publish_date column : datetime형식으로 변환 </b>

~~~python
import pandas as pd
import json
import os

dir_name = '~~path~~/brunch_data/json'

def get_file_list(dir_name): # file name들을 가져오는 함수 # 폴더명 인자 # 폴더가 위치한 경로를 인자로
    return os.listdir(dir_name) # 폴더 내 파일명을 리스트 형태로 반환

file_list = get_file_list(dir_name) # 카테고리별 json파일. 총 24개

## keyword column 전처리
def pre_keyword(x):
    tmp = []
    for val in x:
      tmp.append(val.replace("\n","").replace(" ",""))
    return tmp

## comment column 전처리
def pre_comment(x):
    if len(x) == 0:
        return None
    else :
        return x

## text column 전처리
def pre_text(x):
    return str(x)

## publish date column 전처리
def pre_datetime(x):
    x = x.split('T')[0]
    x = pd.to_datetime(x,format="%Y-%m-%d")
    return x

## 카테고리명. 즉, class를 0~19로 mapping할 것임.
class_condition = {'지구한바퀴_세계여행':0 , '그림·웹툰':1, '시사·이슈':2, 'IT_트렌드':3, '사진·촬영':4, '취향저격_영화_리뷰':5,
                   '뮤직_인사이드':6, '육아_이야기':7, '요리·레시피':8, '건강·운동':9, '멘탈_관리_심리_탐구':10, '문화·예술':11, '건축·설계':12,
                   '인문학·철학':13, '쉽게_읽는_역사':14, '우리집_반려동물':15, '글쓰기_코치':16, '오늘은_이런_책':16, '직장인_현실_조언':17, '스타트업_경험담':17,
                   '디자인_스토리':18, '멋진_캘리그래피':18, '사랑·이별':19, '감성_에세이':19}

all_df = pd.DataFrame(columns=['class','text']) # contcat 위한 비어있는 DataFrame
each_df = {}
for file in file_list:
    with open('~~path~~/brunch_data/json/'+file,encoding='UTF8') as json_file:
        json_data = json.load(json_file)
    ## 각 카테고리별 data에 전처리 함수 적용 후 concat
    df = pd.DataFrame(json_data['data'],
                  columns=['title','keyword','text','nickname','publish_date','likes','share','comment','url','url_plink'])
    df = df.dropna(subset=['text'])
    df['keyword'] = df['keyword'].apply(pre_keyword)
    df['comment'] = df['comment'].apply(pre_comment)
    df['text'] = df['text'].apply(pre_text)
    df['publish_date'] = df['publish_date'].apply(pre_datetime)
    df.insert(0,"class",file[:-5])
    df['class'] = df['class'].map(class_condition)

    all_df = pd.concat([all_df,df[['class','title','text','keyword','publish_date','likes','share','comment','url']][:2000]]) # 비어있는 all_df에 각 카테고리별 df concat
    each_df[file[:-5]] = df


all_df = all_df.reset_index(drop=True) # 전체 index 초기화
~~~

<br><br>

<img src = "https://user-images.githubusercontent.com/35517797/80689481-62189980-8b08-11ea-97e6-e223112ad1d5.PNG" height="400" width="720px">

<br><br>

"잔처리"가 완료되어 어느정도 깔끔해진 데이터를 얻은것을 확인할 수 있습니다.

<br>

## <b> 2-2 text 데이터 전처리 </b>

<br>

이제 프로젝트의 핵심인 Text 데이터 전처리를 진행합니다. 현재 수집된 텍스트 데이터는 "https, www, ax0, ... "등의 의미없는 요소들을 제거하고 Text를 머신러닝 알고리즘이 이해할수 있는 형태로 변환하여야 합니다. 이 과정을 크게 3가지 과정으로 진행하겠습니다.

* <b> 정제 </b>
* <b> 토큰화 </b>
* <b> 벡터화 </b>

<br>

### <b> 2-2-1 text 정제(cleaning) </b>

<br>

현재 수집된 text 데이터 일부를 살펴보겠습니다. 아직 정제를 하지 않았기 때문에 xa0, [], '', http, url주소, 특수문자 등 의미없는 요소들이 가득합니다. 우선 의미없는 요소들을 `정규식`을 활용하여 제거하겠습니다. 그 후에 영상이나 이미지를 소개하는게 주목적들인 글들을 제거해줍니다(text 200자 이하인 글들)

<br>

<img src = "https://user-images.githubusercontent.com/35517797/81373183-192f9900-9137-11ea-9ad5-48009b01481c.PNG" height="400" width="720px">

<br><br>

~~~python
import re

# 정규식 적용 함수
def pre_text_2(x):
  pa = re.compile("^\\\\xa0|xa") # xa0, xa 제거
  pa1 = re.compile(r"'http.*?'") # url 제거
  pa2 = re.compile(r'\([^)]*\)') # (), ()사이 문자
  pa3 = re.compile('[^\w\s]') # 특수문자 삭제
  pa4 = re.compile(r'[^a-zA-Zㄱ-힗]') # 한글,영어만 남김

  x = re.sub(pa,' ',x)
  x = re.sub(pa1,' ',x)
  x = re.sub(pa2,' ',x)
  x = re.sub(pa3, ' ',x)
  x = re.sub(pa4, ' ',x)
  x = x.strip()
  x = " ".join(x.split())

  return x

all_df['text'] = all_df['text'].apply(pre_text_2)

### text 200자 이하인 글들. (주로 영상,이미지 자료 올려놓은 글들임. 제거)
del_list = []
for idx,text in enumerate(all_df['text']):
  if len(text) < 200:
    del_list.append(idx)

all_df = all_df.loc[~all_df.index.isin(del_list), :]
~~~

<br>

<img src = "https://user-images.githubusercontent.com/35517797/81410983-9cbfa900-917c-11ea-81d2-f208690170df.PNG" height="400" width="720px">

<br><br>

### <b> 2-2-2 text 토큰화(Tokenize) & Vectorize </b>

<br>

한글 형태소 분석을 위한 tokenizer와 stopwords를 세팅한 후, 피처 벡터화를 진행합니다. 피처 벡터화를 간단히 설명하면 비정형 데이터인 텍스트 데이터를 word(또는 word의 일부분) 기반의 다수의 피처로 추출하고 이 피처에 단어 빈도수와 같은 숫자 값을 부여하여 텍스트 데이터를 단어의 조합인 벡터값으로 표현하는것을 의미합니다. <br>

피처 벡터화 방법에는 크게 2가지(BOW, Word2Vec)의 방법이 있습니다. 이중에서 BOW방식은 문서가 가지는 모든 단어(words)를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 피처값을 추출합니다. 단순히 단어의 발생 횟수에 기반하고 있지만 문서의 특징을 잘 나타낼수 있는 모델이므로 Brunch Networing의 Text Classifier기능 구현을 위한 모델로 BOW 방식의 TF-IDF 방식을 사용하였습니다.

* <b>1. Train/Val/Test 분리 </b>
* <b>2. Mecab Tokenizer </b>
* <b>3. 한국어 불용어 제거  </b>
* <b>4. TF-IDF 피처 벡터화 </b>

~~~python
from sklearn.model_selection import train_test_split
from konlpy.tag import Mecab
from sklearn.feature_extraction.text import TfidfVectorizer

## Train/Validation/Test 분리
X_train,X_test,y_train,y_test = train_test_split(all_df[['text']],all_df['class'],test_size=0.2,random_state=0)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)

## Konlpy Mecab 토크나이저
mecab = Mecab()
def mecab_tokenizer(text):
    tokens_ko = mecab.morphs(text)
    return tokens_ko

# 한국어 불용어
k_stopwords = pd.read_csv("~~path~~/k_stopwords.csv",sep='\t',header=None)
k_stopwords = list(k_stopwords.iloc[:,0])

## TF-IDF 피처 벡터화
tfidf_vect = TfidfVectorizer(tokenizer=mecab_tokenizer, max_df=0.9, stop_words=k_stopwords) # Vectorizer 생성
tfidf_train_matrix = tfidf_vect.fit_transform(X_train['text']) # train 데이터 fit_transform
tfidf_test_matrix = tfidf_vect.transform(X_test['text']) ## test 데이터 transform
tfidf_val_matrix = tfidf_vect.transform(X_val['text']) ## validation 데이터 transform
~~~

<br><br>

## <b> 3. Modeling </b>

<br>

모델 구축 및 파라미터 최적화, 모델 선택,적용 단계를 시작합니다. 우선 빠르게 3가지 모델을 구축하고 평가지표를 살펴보겠습니다.

* Logistic Regression
* SVM
* Naive Bayse

~~~python
## logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix,accuracy_score,f1_score
from sklearn.metrics import classification_report

lg_params = {'C' :[0.1,1,10]} # logistice grid parameter

lg_clf = LogisticRegression(n_jobs=-1) # logistic regressor

grid_lg = GridSearchCV(lg_clf, param_grid=lg_params, cv=3) # 3 fold gridSearch
grid_lg.fit(tfidf_train_matrix,y_train)
print(grid_lg.best_params_, round(grid_lg.best_score_,2))

pred_logistic = grid_lg.predict(tfidf_test_matrix)

print("classification report", classification_report(y_test,pred_logistic))
print("accuracy : ",accuracy_score(y_test,pred_logistic))
print("f1_score : ",f1_score(y_test,pred_logistic, average='macro'))
~~~

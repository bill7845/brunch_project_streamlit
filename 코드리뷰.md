# Brunch Networking Tech
---

<br>

## < 목차 >

[1. 데이터 수집](#데이터-수집) <br> 
[2. 데이터 전처리](#데이터-전처리) <br>
  [2.1. 데이터 클렌징](#데이터-클렌징) <br>
  [2.2. 텍스트 전처리](#텍스트-클렌징) <br>
  [2.3 tokenize & vectorize](#텍스트-토크나이즈&벡터화) <br>
[3. 분류 모델](#분류-모델링) <br>
[4. 추천시스템](#데이터-수집) <br>
[5. streamlit Web App](#streamlit-web-app)

<!--
* 1. 데이터 수집
* 2. 데이터 전처리
  - 2-1. 데이터 클렌징
  - 2-2. 텍스트 데이터 전처리
    - 2-2-1 text cleaning
    - 2-2-2 text tokenize & vectorize
* 3. 분류 모델
  - 평가
* 4. 추천 시스템
  - 4.1 입력 데이터 기반
  - 4.2 키워드 기반
* 5. streamlit을 활용한 API <br>
-->

<br>

## <b><데이터 수집></b>
<br>

Bruch Networking 프로젝트를 진행하며 경험한 데이터 수집-전처리-분석-모델링의 전체 과정을 코드와 함께 설명하고자 합니다.

<br>

<!-- <img src = "https://user-images.githubusercontent.com/35517797/80509879-f1149d00-89b4-11ea-939d-5eb1af734319.png" height="300" width="650px"> -->


먼저, 프로젝트를 위해 필요한 데이터를 정의합니다. 프로젝트의 주된 목표는 `text 자동분류`, `text 추천시스템` 2가지입니다. 이에 맞춰 필요한 데이터를 우선적으로 정의합니다.

<br>

* <b> Brunch에 게시된 게시글(18개 카테고리별) </b>
* <b> 각 게시글 별 정보(제목,발행일,공유횟수,좋아요수,keyword,댓글 ...) </b>

<br>

이렇게 필요 데이터를 정의한 후 크롤링 코드를 구현합니다. 브런치의 카테고리별 게시글page의 경우 "무한스크롤" javaScript가 구현되어 있습니다.
javaScript를 제어하기 위해서는 BeautifulSoup만으로는 불가능하므로 시간이 더 소요될 수 있지만 Selenium패키지를 사용해야 합니다. 전체 크롤링 과정을 요약하면 크게 2단계로 요약할 수 있습니다.

<br>

* <b> 1. 카테고리별 게시글 목록 page => 게시글 별 작가 id(url) parsing </b> <br>
* <b> 2. 각 게시글 page => text, 제목, 발행일 등의 정보를 parsing </b>

<br>

<img src = "https://user-images.githubusercontent.com/35517797/81310986-c5359d80-90bf-11ea-874c-7473a9b910b0.PNG" height="300" width="650px">

<br>
<br>

2단계의 크롤링을 Python 코드로 구현합니다.

<br>

~~~python
###################################################################
########################### 크롤링 1단계 ###########################
#####카테고리별 게시글 목록 page => 게시글 별 작가 id(url) parsing ###
###################################################################
# library import
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import Select
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import NoAlertPresentException
from selenium.webdriver.chrome.options import Options
import time
import requests
import pickle
from bs4 import BeautifulSoup

# selenium 속도 향상 위해 불필요한 옵션을 사용하지 않게하는 코드
options = Options()
prefs = {'profile.default_content_setting_values': {'cookies' : 2, 'images': 2, 
                                                    'plugins' : 2, 'popups': 2, 'geolocation': 2,
                                                    'notifications' : 2, 'auto_select_certificate': 2,
                                                    'fullscreen' : 2,
                                                    'mouselock' : 2, 'mixed_script': 2, 
                                                    'media_stream' : 2, 'media_stream_mic' : 2,
                                                    'media_stream_camera': 2, 'protocol_handlers' : 2,
                                                    'ppapi_broker' : 2, 'automatic_downloads': 2, 'midi_sysex' : 2,
                                                    'push_messaging' : 2, 'ssl_cert_decisions': 2, 'metro_switch_to_desktop' : 2,
                                                    'protected_media_identifier': 2, 'app_banner': 2, 'site_engagement' : 2,
                                                    'durable_storage' : 2}}

options.add_experimental_option('prefs', prefs) 
options.add_argument("start-maximized") 
options.add_argument("disable-infobars") 
options.add_argument("--disable-extensions")

## 카테고리 별 게시글 리스트 페이지에서 유저별 id 파싱 (*전체 18개 카테고리)
## 각 페이지별 무한 스크롤 javaScript 제어를 위한 셀레니움 기능 사용
def get_user_list(base_url):
#     크롬드라이버 설정
    chromedriver = 'C:/selenium/chromedriver.exe' 
    driver = webdriver.Chrome(chromedriver)
    driver.get(base_url) 
    
    SCROLL_PAUSE_TIME = 15 # 무한스크롤 멈춤 현상 방지
    last_height = driver.execute_script("return document.body.scrollHeight")
    i = 0
    while True:
        # Scroll down to bottom                                                      
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        # Wait to load page
        time.sleep(SCROLL_PAUSE_TIME)                                                
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight-50);")  
        time.sleep(SCROLL_PAUSE_TIME)

        # Calculate new scroll height and compare with last scroll height            
        new_height = driver.execute_script("return document.body.scrollHeight")
        i+=1

        if i == 700 :                                            
            break
        last_height = new_height
    
    print(i,"회에서 멈춤")
    
    ## 각 페이별 700번 스크롤 후, BeautifulSoup으로 "전체 게시글 url parsing"
    html = driver.page_source
    soup = BeautifulSoup(html,'html.parser')
    
    # 게시글별 user_id parsing
    a_tags = soup.select('#wrapArticle > div.wrap_article_list.\#keyword_related_contents > ul > li > a.link_post')
    
    save_href = []
    for a_tag in a_tags :
        save_href.append(a_tag['href'])
    
    driver.close()
    
    return save_href # 개별 카테고리별 전체 게시글 url(user_id) 반환

######################### 
category_list = ['지구한바퀴_세계여행?q=g','시사·이슈?q=g','IT_트렌드?q=g',
                '취향저격_영화_리뷰?q=g','오늘은_이런_책?q=g','뮤직_인사이드?q=g',
                 '직장인_현실_조언?q=g','스타트업_경험담?q=g','육아_이야기?q=g',
                 '요리·레시피?q=g','건강·운동?q=g','멘탈_관리_심리_탐구?q=g',
                 '문화·예술?q=g','인문학·철학?q=g','쉽게_읽는_역사?q=g',
                '우리집_반려동물?q=g','사랑·이별?q=g','감성_에세이?q=g']


# 전체 18개 카테고리별로 돌며 게시글 url 파싱하여 pickle로 저장
for category in category_list:
    each_user_id = []
    each_user_id = get_user_list("https://brunch.co.kr/keyword/"+category)
    with open(category[:-4]+'_userId.txt','wb') as f: # pickle로 저장
        pickle.dump(each_user_id,f)
~~~

<br>

1단계 크롤링의 결과로 18개 카테고리별 게시글 url(user_id)을 담은 리스트를 얻었습니다. 이제 각 게시글 정보를 수집하여 json 파일로 저장합니다.

<br>

~~~python
###################################################################
########################### 크롤링 2단계 ###########################
###   각 게시글 page => text, 제목, 발행일 등의 정보를 parsing ######
###################################################################
import pandas as pd
import pickle
import requests
import json
import re
from tqdm import tqdm
from bs4 import BeautifulSoup

## url pickle load
pickles = ['지구한바퀴_세계여행?q=g','시사·이슈?q=g','IT_트렌드?q=g',
                '취향저격_영화_리뷰?q=g','오늘은_이런_책?q=g','뮤직_인사이드?q=g',
                 '직장인_현실_조언?q=g','스타트업_경험담?q=g','육아_이야기?q=g',
                 '요리·레시피?q=g','건강·운동?q=g','멘탈_관리_심리_탐구?q=g',
                 '문화·예술?q=g','인문학·철학?q=g','쉽게_읽는_역사?q=g',
                '우리집_반려동물?q=g','사랑·이별?q=g','감성_에세이?q=g']


writer_list = []
for file in pickles:
    with open(file[:-4]+"_userId.txt","rb") as fr:
        writers = pickle.load(fr)
    writer_list.append(writers)  ## [[카테고리1 게시글 url...],[카테고리2 게시글 url], ....[카테고리24 게시글 url]]

## 게시글 속 정보 수집
def def_craw(writer):
    
    json_data = {}
    data = []
    res_text = []
    tag_keyword=[]
    
    tag_title,tag_nickname,tag_publish_date,tag_url,tag_url_plink = None,None,None,None,None
    tag_share,tag_like = str,str
    for idx,url in enumerate(writer):
        if idx % 500 == 0 : print("전체",len(writer),"중에",idx)
        if res_text == []: # 첫 시작에러 방지
            pass
        else :
            # to json
            json_data['title'] = tag_title  
            json_data['nickname'] = tag_nickname
            json_data['publish_date'] = tag_publish_date
            json_data['keyword'] = tmp_keyword   
            json_data['like'] = tag_like # like 없는 경우 ''
            json_data['share'] = tag_share # share 없는 경우 None            
            json_data['comment'] = tag_comment # comment 없는 경우 ''
            json_data['url'] = tag_url
            json_data['url_plink'] = tag_url_plink 
            json_data['text'] = res_text

        data.append(json_data)
        
        json_data = {} # 누적방지 초기화
        tmp_keyword = [] # 누적방지 초기화
        res_text = [] # 누적방지 초기화 
        
        # beautifulsoup
        html = requests.get('https://brunch.co.kr{text_url}'.format(text_url=url))
        soup = BeautifulSoup(html.text, 'html.parser')
        
        if soup.find('title').text == "brunch":
            pass
        else:
            tag_title = soup.find('title').text # 게시글 title
            tag_url = soup.find("meta",property='og:url')['content'] # 게시글 본주소
            tag_nickname = soup.find("meta",{'name':'article:media_name'})['content'] # 작가 nickname
            tag_url_plink = soup.find("meta",property='dg:plink')['content'] # 암호주소? # 모바일?
            tag_publish_date = soup.find("meta",property='article:published_time')['content'] # 발행일
            tag_keyword = soup.find_all('a',href=re.compile('/keyword')) # 게시글 키워드
            tag_like = soup.find('span',{'class':'f_l text_like_count text_default text_with_img_ico ico_likeit_like #like'}) #좋아요 수
            tag_share = soup.find('span',{'class':'f_l text_share_count text_default text_with_img_ico'}) # 공유 수
            tag_comment = soup.find('span',{'class':'f_l text_comment_count text_default text_with_img_ico'}) # 댓글 수
            text_h4 = soup.find_all(class_='wrap_item item_type_text')
            
            for text in text_h4:
                res_text.append(text.text)
    
            if tag_like == None:
                tag_like = "0"
            else:
                tag_like = tag_like.text # 좋아요 수

            if tag_share == None:
                tag_share == "0"
            else:
                tag_share = tag_share.text # 공유 수

            if tag_comment == None:
                tag_comment =="0"
            else:
                tag_comment = tag_comment.text

            for keyword in tag_keyword:
                tmp_keyword.append(keyword.text)
                
    return data ## 수집한 정보를 담은 dictionary로 반환

categories = ['지구한바퀴_세계여행','시사_이슈','IT_트렌드',
                '취항저격_영화리뷰','오늘은_이런책','뮤직_인사이드',
                 '직장인_현실조언','스타트업_경험담','육아_이야기',
                 '요리_레시피','건강_운동','멘탈관리_심리탐구',
                 '문화_예술','인문학_철학','쉽게_읽는_역사',
                '우리집_반려동물','사랑_이별','감성_에세이']


## 카테고리 -> 게시글의 순서로 2차 크롤링 진행
## 카테고리별로 정보를 담은 json 형식으로 저장(총 18개 json file)
from collections import OrderedDict
for idx,writer in enumerate(writer_list):
    to_json = None
    data = def_craw(writer) # 2단계 크롤링 실행
    
    del data[0]
    del data[0]
    
    to_json = OrderedDict()
    to_json['name'] = categories[idx] # category name
    to_json['version'] = "2020-06-01"
    to_json['data'] = data
    
    with open(categories[idx]+".json","w") as make_file:
        json.dump(to_json,make_file)
~~~

<br>

이제 필요한 데이터 수집이 완료되었습니다.  수집한 데이터는 json 형식으로 저장되어 있으며 일부를 확인해 보겠습니다.

<br>

![json 데이터 일부](https://user-images.githubusercontent.com/35517797/83967748-335bc300-a8ff-11ea-9b65-a61e5364aa5b.PNG)


<br><br>

## <b><데이터 전처리></b>

<br>

크롤링하여 json형식으로 저장한 데이터를 분석에 용이하도록 Pandas의 DataFrame형식으로 변환한 후에 결측값, 공백 제거 등 기본적인 전처리들을 진행합니다.

* <b> text column : 결측값 삭제 , 기존 문장단위의 리스트 형식에서 전체 문자열 형식으로 변환 </b>
* <b> keyword column : \n, 공백 제거 후 리스트 형식으로 변환 </b>
* <b> comment column : comment가 없는경우 공백이 아닌 Nan으로 변환 </b>
* <b> publish_date column : datetime형식으로 변환 </b>

~~~python
import pandas as pd
import json
import os

dir_name = '~~path~~/brunch_data/json'

def get_file_list(dir_name): # file name들을 가져오는 함수 # 폴더명 인자 # 폴더가 위치한 경로를 인자로
    return os.listdir(dir_name) # 폴더 내 파일명을 리스트 형태로 반환

file_list = get_file_list(dir_name) # 카테고리별 json파일. 총 24개

## keyword column 전처리
def pre_keyword(x):
    tmp = []
    for val in x:
      tmp.append(val.replace("\n","").replace(" ",""))
    return tmp

## comment column 전처리
def pre_comment(x):
    if len(x) == 0:
        return None
    else :
        return x

## text column 전처리
def pre_text(x):
    return str(x)

dir_name = 'C:/Users/KIHyuk/Desktop/brunch_data/json'

def get_file_list(dir_name): # file name들을 가져오는 함수 # 폴더명 인자 # 폴더가 위치한 경로를 인자로
    return os.listdir(dir_name) # 폴더 내 파일명을 리스트 형태로 반환 

file_list = get_file_list(dir_name)

def pre_keyword(x):
    tmp = []
    for val in x:
        tmp.append(val.replace("\n","").replace(" ",""))
    return tmp

def pre_comment(x):
    if len(x) == 0:
        return None
    else :
        return x

def pre_text(x):
    return str(x)

def pre_datetime(x):
    x = x.split('T')[0]
    x = pd.to_datetime(x,format="%Y-%m-%d")
    return x

all_df = pd.DataFrame(columns=['class','text'])
each_df = {}
class_name = []

for file in file_list:
    with open('C:/Users/KIHyuk/Desktop/brunch_data/json/'+file,encoding='UTF8') as json_file:
        json_data = json.load(json_file)

    class_name.append(file[:-5])
    
    df = pd.DataFrame(json_data['data'],
                  columns=['title','keyword','text','nickname','publish_date','likes','share','comment','url','url_plink'])
    df = df.dropna(subset=['text'])
    df['keyword'] = df['keyword'].apply(pre_keyword)
    df['comment'] = df['comment'].apply(pre_comment)
    df['text'] = df['text'].apply(pre_text)
    df['publish_date'] = df['publish_date'].apply(pre_datetime)
    df.insert(0,"class",file[:-5])

    all_df = pd.concat([all_df,df[['class','title','text','keyword','likes','share','comment','publish_date','url']]])
    each_df[file[:-5]] = df


to_categorical = [i for i in range(18)]
class_condition = {}
for a,b in zip(class_name,to_categorical):
    class_condition[a] = b

all_df['ori_class'] = all_df['class']
all_df['class'] = all_df['class'].map(class_condition)
all_df = all_df.reset_index(drop=True)

import re
def pre_text_2(x):
    pa = re.compile("^\\\\xa0|xa")
    pa1 = re.compile(r"'http.*?'") # 전체 url 제거
    pa2 = re.compile(r'\([^)]*\)') # () 사이 문자 
    pa3 = re.compile('[^\w\s]') # 특수문자 삭제
    pa4 = re.compile(r'[^a-zA-Zㄱ-힗]')

    x = re.sub(pa,' ',x)
    x = re.sub(pa1,' ',x)
    x = re.sub(pa2,' ',x)
    x = re.sub(pa3, ' ',x)
    x = re.sub(pa4, ' ',x)
    x = x.strip()
    x = " ".join(x.split())
    return x 

all_df['text'] = all_df['text'].apply(pre_text_2)

## 글 길이가 500자 이하인 게시글 제거
del_list = []
for idx,text in enumerate(all_df['text']):
    if len(text) < 500:
        del_list.append(idx)

all_df = all_df.loc[~all_df.index.isin(del_list), :]

## 중복 문서 제거
print("전체 문서 : ", len(all_df['text']))
print("중복 문서 : ", len(all_df['text']) - all_df['text'].nunique())

all_df.drop_duplicates(subset=['text'], inplace=True) # 중복문서 제거
print("중복 제거 후 전체 문서 :", len(all_df['text']))

all_df = all_df.reset_index(drop=True) # index 초기화 
~~~

<br><br>

![카테고리별 분포](https://user-images.githubusercontent.com/35517797/83969640-095ccd80-a90c-11ea-8f96-03bbbb47ae04.png)


<br><br>

"잔처리"가 완료되어 어느정도 깔끔해진 데이터를 얻은것을 확인할 수 있습니다.

<br>

## <b> 2-2 text 데이터 전처리 </b>

<br>

이제 프로젝트의 핵심인 Text 데이터 전처리를 진행합니다. 현재 수집된 텍스트 데이터는 "https, www, ax0, ... "등의 의미없는 요소들을 제거하고 Text를 머신러닝 알고리즘이 이해할수 있는 형태로 변환하여야 합니다. 이 과정을 크게 3가지 과정으로 진행하겠습니다.

* <b> 정제 </b>
* <b> 토큰화 </b>
* <b> 벡터화 </b>

<br>

### <b> 2-2-1 text 정제(cleaning) </b>

<br>

현재 수집된 text 데이터 일부를 살펴보겠습니다. 아직 정제를 하지 않았기 때문에 xa0, [], '', http, url주소, 특수문자 등 의미없는 요소들이 가득합니다. 우선 의미없는 요소들을 `정규식`을 활용하여 제거하겠습니다. 그 후에 영상이나 이미지를 소개하는게 주목적들인 글들을 제거해줍니다(text 200자 이하인 글들)

<br>

<img src = "https://user-images.githubusercontent.com/35517797/81373183-192f9900-9137-11ea-9ad5-48009b01481c.PNG" height="400" width="720px">

<br><br>

~~~python
import re

# 정규식 적용 함수
def pre_text_2(x):
  pa = re.compile("^\\\\xa0|xa") # xa0, xa 제거
  pa1 = re.compile(r"'http.*?'") # url 제거
  pa2 = re.compile(r'\([^)]*\)') # (), ()사이 문자
  pa3 = re.compile('[^\w\s]') # 특수문자 삭제
  pa4 = re.compile(r'[^a-zA-Zㄱ-힗]') # 한글,영어만 남김

  x = re.sub(pa,' ',x)
  x = re.sub(pa1,' ',x)
  x = re.sub(pa2,' ',x)
  x = re.sub(pa3, ' ',x)
  x = re.sub(pa4, ' ',x)
  x = x.strip()
  x = " ".join(x.split())

  return x

all_df['text'] = all_df['text'].apply(pre_text_2)

### text 200자 이하인 글들. (주로 영상,이미지 자료 올려놓은 글들임. 제거)
del_list = []
for idx,text in enumerate(all_df['text']):
  if len(text) < 200:
    del_list.append(idx)

all_df = all_df.loc[~all_df.index.isin(del_list), :]
~~~

<br>

<img src = "https://user-images.githubusercontent.com/35517797/81410983-9cbfa900-917c-11ea-81d2-f208690170df.PNG" height="400" width="720px">

<br><br>

### <b> 2-2-2 text 토큰화(Tokenize) & Vectorize </b>

<br>

한글 형태소 분석을 위한 tokenizer와 stopwords를 세팅한 후, 피처 벡터화를 진행합니다. 피처 벡터화를 간단히 설명하면 비정형 데이터인 텍스트 데이터를 word(또는 word의 일부분) 기반의 다수의 피처로 추출하고 이 피처에 단어 빈도수와 같은 숫자 값을 부여하여 텍스트 데이터를 단어의 조합인 벡터값으로 표현하는것을 의미합니다. <br>

피처 벡터화 방법에는 크게 2가지(BOW, Word2Vec)의 방법이 있습니다. 이중에서 BOW방식은 문서가 가지는 모든 단어(words)를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 피처값을 추출합니다. 단순히 단어의 발생 횟수에 기반하고 있지만 문서의 특징을 잘 나타낼수 있는 모델이므로 Brunch Networing의 Text Classifier기능 구현을 위한 모델로 BOW 방식의 TF-IDF 방식을 사용하였습니다.

* <b>1. Train/Val/Test 분리 </b>
* <b>2. Mecab Tokenizer </b>
* <b>3. 한국어 불용어 제거  </b>
* <b>4. TF-IDF 피처 벡터화 </b>

~~~python
from sklearn.model_selection import train_test_split
from konlpy.tag import Mecab
from sklearn.feature_extraction.text import TfidfVectorizer

## Train/Validation/Test 분리
X_train,X_test,y_train,y_test = train_test_split(all_df[['text']],all_df['class'],test_size=0.2,random_state=0)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)

## Konlpy Mecab 토크나이저
mecab = Mecab()
def mecab_tokenizer(text):
    tokens_ko = mecab.morphs(text)
    return tokens_ko

# 한국어 불용어
k_stopwords = pd.read_csv("~~path~~/k_stopwords.csv",sep='\t',header=None)
k_stopwords = list(k_stopwords.iloc[:,0])

## TF-IDF 피처 벡터화
tfidf_vect = TfidfVectorizer(tokenizer=mecab_tokenizer, max_df=0.9, stop_words=k_stopwords) # Vectorizer 생성
tfidf_train_matrix = tfidf_vect.fit_transform(X_train['text']) # train 데이터 fit_transform
tfidf_test_matrix = tfidf_vect.transform(X_test['text']) ## test 데이터 transform
tfidf_val_matrix = tfidf_vect.transform(X_val['text']) ## validation 데이터 transform
~~~

<br><br>

## <b> 3. Modeling </b>

<br>

모델 구축 및 파라미터 최적화, 모델 선택,적용 단계를 시작합니다. 우선 빠르게 3가지 모델을 구축하고 평가지표를 살펴보겠습니다.

* Logistic Regression
* SVM
* Naive Bayse

~~~python
## logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix,accuracy_score,f1_score
from sklearn.metrics import classification_report

lg_params = {'C' :[0.1,1,10]} # logistice grid parameter

lg_clf = LogisticRegression(n_jobs=-1) # logistic regressor

grid_lg = GridSearchCV(lg_clf, param_grid=lg_params, cv=3) # 3 fold gridSearch
grid_lg.fit(tfidf_train_matrix,y_train)
print(grid_lg.best_params_, round(grid_lg.best_score_,2))

pred_logistic = grid_lg.predict(tfidf_test_matrix)

print("classification report", classification_report(y_test,pred_logistic))
print("accuracy : ",accuracy_score(y_test,pred_logistic))
print("f1_score : ",f1_score(y_test,pred_logistic, average='macro'))
~~~

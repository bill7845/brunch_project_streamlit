# Brunch Networking Tech

<br>

## < 목차 >

[1. 데이터 수집](#데이터-수집) <br> 
[2. 데이터 전처리](#데이터-전처리) <br>
[3. tokenize & vectorize](#텍스트-토크나이즈&벡터화) <br>
[4. 분류 모델](#분류-모델링) <br>
[5. 추천시스템](#데이터-수집) <br>
[6. streamlit Web App](#streamlit-web-app)

<!--
* 1. 데이터 수집
* 2. 데이터 전처리
  - 2-1. 데이터 클렌징
  - 2-2. 텍스트 데이터 전처리
    - 2-2-1 text cleaning
    - 2-2-2 text tokenize & vectorize
* 3. 분류 모델
  - 평가
* 4. 추천 시스템
  - 4.1 입력 데이터 기반
  - 4.2 키워드 기반
* 5. streamlit을 활용한 API <br>
-->

<br>

## <b><데이터 수집></b>
<br>

Bruch Networking 프로젝트를 진행하며 경험한 데이터 수집-전처리-분석-모델링의 전체 과정을 코드와 함께 설명하고자 합니다.

<br>

<!-- <img src = "https://user-images.githubusercontent.com/35517797/80509879-f1149d00-89b4-11ea-939d-5eb1af734319.png" height="300" width="650px"> -->


먼저, 프로젝트를 위해 필요한 데이터를 정의합니다. 프로젝트의 주된 목표는 `text 자동분류`, `text 추천시스템` 2가지입니다. 이에 맞춰 필요한 데이터를 우선적으로 정의합니다.

<br>

* <b> Brunch에 게시된 게시글(18개 카테고리별) </b>
* <b> 각 게시글 별 정보(제목,발행일,공유횟수,좋아요수,keyword,댓글 ...) </b>

<br>

이렇게 필요 데이터를 정의한 후 크롤링 코드를 구현합니다. 브런치의 카테고리별 게시글page의 경우 "무한스크롤" javaScript가 구현되어 있습니다.
javaScript를 제어하기 위해서는 BeautifulSoup만으로는 불가능하므로 시간이 더 소요될 수 있지만 Selenium패키지를 사용해야 합니다. 전체 크롤링 과정을 요약하면 크게 2단계로 요약할 수 있습니다.

<br>

* <b> 1. 카테고리별 게시글 목록 page => 게시글 별 작가 id(url) parsing </b> <br>
* <b> 2. 각 게시글 page => text, 제목, 발행일 등의 정보를 parsing </b>

<br>

<img src = "https://user-images.githubusercontent.com/35517797/81310986-c5359d80-90bf-11ea-874c-7473a9b910b0.PNG" height="300" width="650px">

<br>
<br>

2단계의 크롤링을 Python 코드로 구현합니다.

<br>

~~~python
###################################################################
########################### 크롤링 1단계 ###########################
#####카테고리별 게시글 목록 page => 게시글 별 작가 id(url) parsing ###
###################################################################
# library import
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import Select
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import NoAlertPresentException
from selenium.webdriver.chrome.options import Options
import time
import requests
import pickle
from bs4 import BeautifulSoup

# selenium 속도 향상 위해 불필요한 옵션을 사용하지 않게하는 코드
options = Options()
prefs = {'profile.default_content_setting_values': {'cookies' : 2, 'images': 2, 
                                                    'plugins' : 2, 'popups': 2, 'geolocation': 2,
                                                    'notifications' : 2, 'auto_select_certificate': 2,
                                                    'fullscreen' : 2,
                                                    'mouselock' : 2, 'mixed_script': 2, 
                                                    'media_stream' : 2, 'media_stream_mic' : 2,
                                                    'media_stream_camera': 2, 'protocol_handlers' : 2,
                                                    'ppapi_broker' : 2, 'automatic_downloads': 2, 'midi_sysex' : 2,
                                                    'push_messaging' : 2, 'ssl_cert_decisions': 2, 'metro_switch_to_desktop' : 2,
                                                    'protected_media_identifier': 2, 'app_banner': 2, 'site_engagement' : 2,
                                                    'durable_storage' : 2}}

options.add_experimental_option('prefs', prefs) 
options.add_argument("start-maximized") 
options.add_argument("disable-infobars") 
options.add_argument("--disable-extensions")

## 카테고리 별 게시글 리스트 페이지에서 유저별 id 파싱 (*전체 18개 카테고리)
## 각 페이지별 무한 스크롤 javaScript 제어를 위한 셀레니움 기능 사용
def get_user_list(base_url):
#     크롬드라이버 설정
    chromedriver = 'C:/selenium/chromedriver.exe' 
    driver = webdriver.Chrome(chromedriver)
    driver.get(base_url) 
    
    SCROLL_PAUSE_TIME = 15 # 무한스크롤 멈춤 현상 방지
    last_height = driver.execute_script("return document.body.scrollHeight")
    i = 0
    while True:
        # Scroll down to bottom                                                      
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        # Wait to load page
        time.sleep(SCROLL_PAUSE_TIME)                                                
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight-50);")  
        time.sleep(SCROLL_PAUSE_TIME)

        # Calculate new scroll height and compare with last scroll height            
        new_height = driver.execute_script("return document.body.scrollHeight")
        i+=1

        if i == 700 :                                            
            break
        last_height = new_height
    
    print(i,"회에서 멈춤")
    
    ## 각 페이별 700번 스크롤 후, BeautifulSoup으로 "전체 게시글 url parsing"
    html = driver.page_source
    soup = BeautifulSoup(html,'html.parser')
    
    # 게시글별 user_id parsing
    a_tags = soup.select('#wrapArticle > div.wrap_article_list.\#keyword_related_contents > ul > li > a.link_post')
    
    save_href = []
    for a_tag in a_tags :
        save_href.append(a_tag['href'])
    
    driver.close()
    
    return save_href # 개별 카테고리별 전체 게시글 url(user_id) 반환

######################### 
category_list = ['지구한바퀴_세계여행?q=g','시사·이슈?q=g','IT_트렌드?q=g',
                '취향저격_영화_리뷰?q=g','오늘은_이런_책?q=g','뮤직_인사이드?q=g',
                 '직장인_현실_조언?q=g','스타트업_경험담?q=g','육아_이야기?q=g',
                 '요리·레시피?q=g','건강·운동?q=g','멘탈_관리_심리_탐구?q=g',
                 '문화·예술?q=g','인문학·철학?q=g','쉽게_읽는_역사?q=g',
                '우리집_반려동물?q=g','사랑·이별?q=g','감성_에세이?q=g']


# 전체 18개 카테고리별로 돌며 게시글 url 파싱하여 pickle로 저장
for category in category_list:
    each_user_id = []
    each_user_id = get_user_list("https://brunch.co.kr/keyword/"+category)
    with open(category[:-4]+'_userId.txt','wb') as f: # pickle로 저장
        pickle.dump(each_user_id,f)
~~~

<br>

1단계 크롤링의 결과로 18개 카테고리별 게시글 url(user_id)을 담은 리스트를 얻었습니다. 이제 각 게시글 정보를 수집하여 json 파일로 저장합니다.

<br>

~~~python
###################################################################
########################### 크롤링 2단계 ###########################
###   각 게시글 page => text, 제목, 발행일 등의 정보를 parsing ######
###################################################################
import pandas as pd
import pickle
import requests
import json
import re
from tqdm import tqdm
from bs4 import BeautifulSoup

## url pickle load
pickles = ['지구한바퀴_세계여행?q=g','시사·이슈?q=g','IT_트렌드?q=g',
                '취향저격_영화_리뷰?q=g','오늘은_이런_책?q=g','뮤직_인사이드?q=g',
                 '직장인_현실_조언?q=g','스타트업_경험담?q=g','육아_이야기?q=g',
                 '요리·레시피?q=g','건강·운동?q=g','멘탈_관리_심리_탐구?q=g',
                 '문화·예술?q=g','인문학·철학?q=g','쉽게_읽는_역사?q=g',
                '우리집_반려동물?q=g','사랑·이별?q=g','감성_에세이?q=g']


writer_list = []
for file in pickles:
    with open(file[:-4]+"_userId.txt","rb") as fr:
        writers = pickle.load(fr)
    writer_list.append(writers)  ## [[카테고리1 게시글 url...],[카테고리2 게시글 url], ....[카테고리24 게시글 url]]

## 게시글 속 정보 수집
def def_craw(writer):
    
    json_data = {}
    data = []
    res_text = []
    tag_keyword=[]
    
    tag_title,tag_nickname,tag_publish_date,tag_url,tag_url_plink = None,None,None,None,None
    tag_share,tag_like = str,str
    for idx,url in enumerate(writer):
        if idx % 500 == 0 : print("전체",len(writer),"중에",idx)
        if res_text == []: # 첫 시작에러 방지
            pass
        else :
            # to json
            json_data['title'] = tag_title  
            json_data['nickname'] = tag_nickname
            json_data['publish_date'] = tag_publish_date
            json_data['keyword'] = tmp_keyword   
            json_data['like'] = tag_like # like 없는 경우 ''
            json_data['share'] = tag_share # share 없는 경우 None            
            json_data['comment'] = tag_comment # comment 없는 경우 ''
            json_data['url'] = tag_url
            json_data['url_plink'] = tag_url_plink 
            json_data['text'] = res_text

        data.append(json_data)
        
        json_data = {} # 누적방지 초기화
        tmp_keyword = [] # 누적방지 초기화
        res_text = [] # 누적방지 초기화 
        
        # beautifulsoup
        html = requests.get('https://brunch.co.kr{text_url}'.format(text_url=url))
        soup = BeautifulSoup(html.text, 'html.parser')
        
        if soup.find('title').text == "brunch":
            pass
        else:
            tag_title = soup.find('title').text # 게시글 title
            tag_url = soup.find("meta",property='og:url')['content'] # 게시글 본주소
            tag_nickname = soup.find("meta",{'name':'article:media_name'})['content'] # 작가 nickname
            tag_url_plink = soup.find("meta",property='dg:plink')['content'] # 암호주소? # 모바일?
            tag_publish_date = soup.find("meta",property='article:published_time')['content'] # 발행일
            tag_keyword = soup.find_all('a',href=re.compile('/keyword')) # 게시글 키워드
            tag_like = soup.find('span',{'class':'f_l text_like_count text_default text_with_img_ico ico_likeit_like #like'}) #좋아요 수
            tag_share = soup.find('span',{'class':'f_l text_share_count text_default text_with_img_ico'}) # 공유 수
            tag_comment = soup.find('span',{'class':'f_l text_comment_count text_default text_with_img_ico'}) # 댓글 수
            text_h4 = soup.find_all(class_='wrap_item item_type_text')
            
            for text in text_h4:
                res_text.append(text.text)
    
            if tag_like == None:
                tag_like = "0"
            else:
                tag_like = tag_like.text # 좋아요 수

            if tag_share == None:
                tag_share == "0"
            else:
                tag_share = tag_share.text # 공유 수

            if tag_comment == None:
                tag_comment =="0"
            else:
                tag_comment = tag_comment.text

            for keyword in tag_keyword:
                tmp_keyword.append(keyword.text)
                
    return data ## 수집한 정보를 담은 dictionary로 반환

categories = ['지구한바퀴_세계여행','시사_이슈','IT_트렌드',
                '취항저격_영화리뷰','오늘은_이런책','뮤직_인사이드',
                 '직장인_현실조언','스타트업_경험담','육아_이야기',
                 '요리_레시피','건강_운동','멘탈관리_심리탐구',
                 '문화_예술','인문학_철학','쉽게_읽는_역사',
                '우리집_반려동물','사랑_이별','감성_에세이']


## 카테고리 -> 게시글의 순서로 2차 크롤링 진행
## 카테고리별로 정보를 담은 json 형식으로 저장(총 18개 json file)
from collections import OrderedDict
for idx,writer in enumerate(writer_list):
    to_json = None
    data = def_craw(writer) # 2단계 크롤링 실행
    
    del data[0]
    del data[0]
    
    to_json = OrderedDict()
    to_json['name'] = categories[idx] # category name
    to_json['version'] = "2020-06-01"
    to_json['data'] = data
    
    with open(categories[idx]+".json","w") as make_file:
        json.dump(to_json,make_file)
~~~

<br>

이제 필요한 데이터 수집이 완료되었습니다.  수집한 데이터는 json 형식으로 저장되어 있으며 일부를 확인해 보겠습니다.

<br>

![json 데이터 일부](https://user-images.githubusercontent.com/35517797/83967748-335bc300-a8ff-11ea-9b65-a61e5364aa5b.PNG)


<br><br>

## <b><데이터 전처리></b>

<br>

크롤링하여 json형식으로 저장한 데이터를 분석에 용이하도록 Pandas의 DataFrame형식으로 변환한 후에 결측값, 공백 제거 등 기본적인 전처리들을 진행합니다.

* <b> text column : 결측값 삭제 , 기존 문장단위의 리스트 형식에서 전체 문자열 형식으로 변환 </b>
* <b> keyword column : \n, 공백 제거 후 리스트 형식으로 변환 </b>
* <b> comment column : comment가 없는경우 공백이 아닌 Nan으로 변환 </b>
* <b> publish_date column : datetime형식으로 변환 </b>

이후에는 게시글의 길이가 500이하인것들과 중복된 게시글을 삭제합니다. 또, 정규식을 활용하여 text의 url, 쉼표, 개행문자 등을 제거합니다.

~~~python
dir_name = '~~path~~/json'

def get_file_list(dir_name): # file name들을 가져오는 함수 # 폴더명 인자 # 폴더가 위치한 경로를 인자로
    return os.listdir(dir_name) # 폴더 내 파일명을 리스트 형태로 반환 

file_list = get_file_list(dir_name)

# \n, 공백 제거 후 리스트 형식으로 변환
def pre_keyword(x):
    tmp = []
    for val in x:
        tmp.append(val.replace("\n","").replace(" ",""))
    return tmp

# comment가 없는경우 공백이 아닌 Nan으로 변환
def pre_comment(x):
    if len(x) == 0:
        return None
    else :
        return x
    
#  문자열로 변환
def pre_text(x):
    return str(x)

# datetime형식으로 변환
def pre_datetime(x):
    x = x.split('T')[0]
    x = pd.to_datetime(x,format="%Y-%m-%d")
    return x

all_df = pd.DataFrame(columns=['class','text'])
each_df = {}
class_name = []
for file in file_list:
    with open(~~path~~'+file,encoding='UTF8') as json_file:
        json_data = json.load(json_file)

    class_name.append(file[:-5])
    
    ## json -> DataFrame
    df = pd.DataFrame(json_data['data'],
                  columns=['title','keyword','text','nickname','publish_date','likes','share','comment','url','url_plink'])
    df = df.dropna(subset=['text'])
    df['keyword'] = df['keyword'].apply(pre_keyword)
    df['comment'] = df['comment'].apply(pre_comment)
    df['text'] = df['text'].apply(pre_text)
    df['publish_date'] = df['publish_date'].apply(pre_datetime)
    df.insert(0,"class",file[:-5])

    all_df = pd.concat([all_df,df[['class','title','text','keyword','likes','share','comment','publish_date','url']]])
    each_df[file[:-5]] = df

## 카테고리별 이름에서 0~18로 label encoding
to_categorical = [i for i in range(18)]
class_condition = {}
for a,b in zip(class_name,to_categorical):
    class_condition[a] = b

all_df['ori_class'] = all_df['class']
all_df['class'] = all_df['class'].map(class_condition)
all_df = all_df.reset_index(drop=True)

import re
def pre_text_2(x):
    pa = re.compile("^\\\\xa0|xa") # xla 등 불용어 제거
    pa1 = re.compile(r"'http.*?'") # 전체 url 제거
    pa2 = re.compile(r'\([^)]*\)') # () 사이 문자 
    pa3 = re.compile('[^\w\s]') # 특수문자 삭제
    pa4 = re.compile(r'[^a-zA-Zㄱ-힗]') # 한글,영어만 남김

    x = re.sub(pa,' ',x)
    x = re.sub(pa1,' ',x)
    x = re.sub(pa2,' ',x)
    x = re.sub(pa3, ' ',x)
    x = re.sub(pa4, ' ',x)
    x = x.strip()
    x = " ".join(x.split())
    return x 

all_df['text'] = all_df['text'].apply(pre_text_2)

## 글 길이 500이하 제거
del_list = []
for idx,text in enumerate(all_df['text']):
    if len(text) < 500:
        del_list.append(idx)

all_df = all_df.loc[~all_df.index.isin(del_list), :]

print("전체 문서 : ", len(all_df['text']))
print("중복 문서 : ", len(all_df['text']) - all_df['text'].nunique())

all_df.drop_duplicates(subset=['text'], inplace=True) # 중복문서 제거
print("중복 제거 후 전체 문서 :", len(all_df['text']))

all_df = all_df.reset_index(drop=True) # index 초기화 
~~~

<br><br>

![카테고리별 분포](https://user-images.githubusercontent.com/35517797/83969673-4923b500-a90c-11ea-8543-08b01677823f.png)

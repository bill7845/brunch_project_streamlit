{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. UserId (게시글별 ID) 크롤러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library import\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import NoAlertPresentException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import requests\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# selenium 속도 향상 위해 불필요한 옵션을 사용하지 않게하는 코드\n",
    "options = Options()\n",
    "prefs = {'profile.default_content_setting_values': {'cookies' : 2, 'images': 2, \n",
    "                                                    'plugins' : 2, 'popups': 2, 'geolocation': 2,\n",
    "                                                    'notifications' : 2, 'auto_select_certificate': 2,\n",
    "                                                    'fullscreen' : 2,\n",
    "                                                    'mouselock' : 2, 'mixed_script': 2, \n",
    "                                                    'media_stream' : 2, 'media_stream_mic' : 2,\n",
    "                                                    'media_stream_camera': 2, 'protocol_handlers' : 2,\n",
    "                                                    'ppapi_broker' : 2, 'automatic_downloads': 2, 'midi_sysex' : 2,\n",
    "                                                    'push_messaging' : 2, 'ssl_cert_decisions': 2, 'metro_switch_to_desktop' : 2,\n",
    "                                                    'protected_media_identifier': 2, 'app_banner': 2, 'site_engagement' : 2,\n",
    "                                                    'durable_storage' : 2}}\n",
    "\n",
    "options.add_experimental_option('prefs', prefs) \n",
    "options.add_argument(\"start-maximized\") \n",
    "options.add_argument(\"disable-infobars\") \n",
    "options.add_argument(\"--disable-extensions\")\n",
    "\n",
    "## 카테고리 별 게시글 리스트 페이지에서 유저별 id 파싱 (*전체 18개 카테고리)\n",
    "## 각 페이지별 무한 스크롤 javaScript 제어를 위한 셀레니움 기능 사용\n",
    "def get_user_list(base_url):\n",
    "#     크롬드라이버 설정\n",
    "    chromedriver = 'C:/selenium/chromedriver.exe' \n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    driver.get(base_url) \n",
    "    \n",
    "    SCROLL_PAUSE_TIME = 15 # 무한스크롤 멈춤 현상 방지\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    i = 0\n",
    "    while True:\n",
    "        # Scroll down to bottom                                                      \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)                                                \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight-50);\")  \n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height            \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        i+=1\n",
    "\n",
    "        if i == 700 :                                            \n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    print(i,\"회에서 멈춤\")\n",
    "    \n",
    "    ## 각 페이별 700번 스크롤 후, BeautifulSoup으로 \"전체 게시글 url parsing\"\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # 게시글별 user_id parsing\n",
    "    a_tags = soup.select('#wrapArticle > div.wrap_article_list.\\#keyword_related_contents > ul > li > a.link_post')\n",
    "    \n",
    "    save_href = []\n",
    "    for a_tag in a_tags :\n",
    "        save_href.append(a_tag['href'])\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return save_href # 개별 카테고리별 전체 게시글 url 반환\n",
    "\n",
    "######################### \n",
    "category_list = ['지구한바퀴_세계여행?q=g','시사·이슈?q=g','IT_트렌드?q=g',\n",
    "                '취향저격_영화_리뷰?q=g','오늘은_이런_책?q=g','뮤직_인사이드?q=g',\n",
    "                 '직장인_현실_조언?q=g','스타트업_경험담?q=g','육아_이야기?q=g',\n",
    "                 '요리·레시피?q=g','건강·운동?q=g','멘탈_관리_심리_탐구?q=g',\n",
    "                 '문화·예술?q=g','인문학·철학?q=g','쉽게_읽는_역사?q=g',\n",
    "                '우리집_반려동물?q=g','사랑·이별?q=g','감성_에세이?q=g']\n",
    "\n",
    "\n",
    "# 전체 18개 카테고리별로 돌며 게시글 url 파싱하여 pickle로 저장\n",
    "for category in category_list:\n",
    "    each_user_id = []\n",
    "    each_user_id = get_user_list(\"https://brunch.co.kr/keyword/\"+category)\n",
    "    with open(category[:-4]+'_userId.txt','wb') as f:\n",
    "        pickle.dump(each_user_id,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 각 게시글별 정보 크롤러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## url pickle load\n",
    "pickles = ['지구한바퀴_세계여행?q=g','시사·이슈?q=g','IT_트렌드?q=g',\n",
    "                '취향저격_영화_리뷰?q=g','오늘은_이런_책?q=g','뮤직_인사이드?q=g',\n",
    "                 '직장인_현실_조언?q=g','스타트업_경험담?q=g','육아_이야기?q=g',\n",
    "                 '요리·레시피?q=g','건강·운동?q=g','멘탈_관리_심리_탐구?q=g',\n",
    "                 '문화·예술?q=g','인문학·철학?q=g','쉽게_읽는_역사?q=g',\n",
    "                '우리집_반려동물?q=g','사랑·이별?q=g','감성_에세이?q=g']\n",
    "\n",
    "\n",
    "writer_list = []\n",
    "for file in pickles:\n",
    "    with open(file[:-4]+\"_userId.txt\",\"rb\") as fr:\n",
    "        writers = pickle.load(fr)\n",
    "    writer_list.append(writers)  ## [[카테고리1 게시글 url...],[카테고리2 게시글 url], ....[카테고리24 게시글 url]]\n",
    "\n",
    "## 게시글 속 정보 수집\n",
    "def def_craw(writer):\n",
    "    \n",
    "    json_data = {}\n",
    "    data = []\n",
    "    res_text = []\n",
    "    tag_keyword=[]\n",
    "    \n",
    "    tag_title,tag_nickname,tag_publish_date,tag_url,tag_url_plink = None,None,None,None,None\n",
    "    tag_share,tag_like = str,str\n",
    "    for idx,url in enumerate(writer):\n",
    "        if idx % 500 == 0 : print(\"전체\",len(writer),\"중에\",idx)\n",
    "        if res_text == []: # 첫 시작에러 방지\n",
    "            pass\n",
    "        else :\n",
    "            # to json\n",
    "            json_data['title'] = tag_title  \n",
    "            json_data['nickname'] = tag_nickname\n",
    "            json_data['publish_date'] = tag_publish_date\n",
    "            json_data['keyword'] = tmp_keyword   \n",
    "            json_data['like'] = tag_like # like 없는 경우 ''\n",
    "            json_data['share'] = tag_share # share 없는 경우 None            \n",
    "            json_data['comment'] = tag_comment # comment 없는 경우 ''\n",
    "            json_data['url'] = tag_url\n",
    "            json_data['url_plink'] = tag_url_plink \n",
    "            json_data['text'] = res_text\n",
    "\n",
    "        data.append(json_data)\n",
    "        \n",
    "        json_data = {} # 누적방지 초기화\n",
    "        tmp_keyword = [] # 누적방지 초기화\n",
    "        res_text = [] # 누적방지 초기화 \n",
    "        \n",
    "        # beautifulsoup\n",
    "        html = requests.get('https://brunch.co.kr{text_url}'.format(text_url=url))\n",
    "        soup = BeautifulSoup(html.text, 'html.parser')\n",
    "        \n",
    "        if soup.find('title').text == \"brunch\":\n",
    "            pass\n",
    "        else:\n",
    "            tag_title = soup.find('title').text # 게시글 title\n",
    "            tag_url = soup.find(\"meta\",property='og:url')['content'] # 게시글 본주소\n",
    "            tag_nickname = soup.find(\"meta\",{'name':'article:media_name'})['content'] # 작가 nickname\n",
    "            tag_url_plink = soup.find(\"meta\",property='dg:plink')['content'] # 암호주소? # 모바일?\n",
    "            tag_publish_date = soup.find(\"meta\",property='article:published_time')['content'] # 발행일\n",
    "            tag_keyword = soup.find_all('a',href=re.compile('/keyword')) # 게시글 키워드\n",
    "            tag_like = soup.find('span',{'class':'f_l text_like_count text_default text_with_img_ico ico_likeit_like #like'}) #좋아요 수\n",
    "            tag_share = soup.find('span',{'class':'f_l text_share_count text_default text_with_img_ico'}) # 공유 수\n",
    "            tag_comment = soup.find('span',{'class':'f_l text_comment_count text_default text_with_img_ico'}) # 댓글 수\n",
    "            text_h4 = soup.find_all(class_='wrap_item item_type_text')\n",
    "            \n",
    "            for text in text_h4:\n",
    "                res_text.append(text.text)\n",
    "    \n",
    "            if tag_like == None:\n",
    "                tag_like = \"0\"\n",
    "            else:\n",
    "                tag_like = tag_like.text # 좋아요 수\n",
    "\n",
    "            if tag_share == None:\n",
    "                tag_share == \"0\"\n",
    "            else:\n",
    "                tag_share = tag_share.text # 공유 수\n",
    "\n",
    "            if tag_comment == None:\n",
    "                tag_comment ==\"0\"\n",
    "            else:\n",
    "                tag_comment = tag_comment.text\n",
    "\n",
    "            for keyword in tag_keyword:\n",
    "                tmp_keyword.append(keyword.text)\n",
    "                \n",
    "    return data ## 수집한 정보를 담은 dictionary로 반환\n",
    "\n",
    "categories = ['지구한바퀴_세계여행','시사_이슈','IT_트렌드',\n",
    "                '취항저격_영화리뷰','오늘은_이런책','뮤직_인사이드',\n",
    "                 '직장인_현실조언','스타트업_경험담','육아_이야기',\n",
    "                 '요리_레시피','건강_운동','멘탈관리_심리탐구',\n",
    "                 '문화_예술','인문학_철학','쉽게_읽는_역사',\n",
    "                '우리집_반려동물','사랑_이별','감성_에세이']\n",
    "\n",
    "\n",
    "## 카테고리 -> 게시글의 순서로 2차 크롤링 진행\n",
    "## 카테고리별로 정보를 담은 json 형식으로 저장(총 24개 json file)\n",
    "from collections import OrderedDict\n",
    "for idx,writer in enumerate(writer_list):\n",
    "    to_json = None\n",
    "    data = def_craw(writer) # 2단계 크롤링 실행\n",
    "    \n",
    "    del data[0]\n",
    "    del data[0]\n",
    "    \n",
    "    to_json = OrderedDict()\n",
    "    to_json['name'] = categories[idx] # category name\n",
    "    to_json['version'] = \"2020-06-01\"\n",
    "    to_json['data'] = data\n",
    "    \n",
    "    with open(categories[idx]+\".json\",\"w\") as make_file:\n",
    "        json.dump(to_json,make_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keyword 크롤러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "category = ['지구한바퀴_세계여행?q=g','시사·이슈?q=g','IT_트렌드?q=g',\n",
    "                '취향저격_영화_리뷰?q=g','오늘은_이런_책?q=g','뮤직_인사이드?q=g',\n",
    "                 '직장인_현실_조언?q=g','스타트업_경험담?q=g','육아_이야기?q=g',\n",
    "                 '요리·레시피?q=g','건강·운동?q=g','멘탈_관리_심리_탐구?q=g',\n",
    "                 '문화·예술?q=g','인문학·철학?q=g','쉽게_읽는_역사?q=g',\n",
    "                '우리집_반려동물?q=g','사랑·이별?q=g','감성_에세이?q=g']\n",
    "\n",
    "name_list = ['지구한바퀴_세계여행','시사_이슈','IT_트렌드','취향저격_영화리뷰',\n",
    "             '오늘은_이런책','뮤직_인사이드','직장인_현실조언','스타트업_경험담',\n",
    "             '육아_이야기','요리_레시피','건강_운동','멘탈관리_심리탐구','문화_예술',\n",
    "             '인문학_철학','쉽게_읽는_역사','우리집_반려동물','사랑_이별','감성_에세이']\n",
    "\n",
    "keyword_dict = {}\n",
    "\n",
    "## 현재 브런치에서 각 카테고리별로 제공하는 keyword 크롤링하여 dict type으로 저장\n",
    "for idx,cate in enumerate(category):\n",
    "    name_list[idx]\n",
    "    html = requests.get('https://brunch.co.kr/keyword/{category_name}'.format(category_name=cate))\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    \n",
    "    target = soup.find_all(class_ = 'keyword_elem')\n",
    "    \n",
    "    keywords = []\n",
    "    for i in target :\n",
    "        keywords.append(i.text)\n",
    "    \n",
    "    keyword_dict[name_list[idx]] = keywords\n",
    "    \n",
    "# 스타트업_경험담 카테고리의 경우에 keyword 정보가 없기 때문에 직접 지정.\n",
    "del(keyword_dict['스타트업_경험담'])\n",
    "keyword_dict['스타트업_경험담'] = ['스타트업','창업','이직','마케팅','조직','캠퍼스','패스트파이브']\n",
    "\n",
    "with open ('keyword_dict.txt','wb') as f:\n",
    "    pickle.dump(keyword_dict,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
